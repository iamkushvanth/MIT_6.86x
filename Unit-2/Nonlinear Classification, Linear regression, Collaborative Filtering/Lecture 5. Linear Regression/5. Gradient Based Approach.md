# **Learning Algorithm: Gradient Based Approach**

**True or False**

Let **R_n(/theta)** be the least squares criterion defined by

**R_ n(\theta )=1/n \sum _{t=1}^{n} \text {Loss}\left(y^{(t)} - \theta \cdot x^{(t)}\right).**

Which of the following is true? Choose all those apply.

[] The least squares criterion **R_n(/theta)** is a sum of functions, one per data point.

[] Each step in stochastic gradient descent requires more computational resources (say, as measured by counting floating point operations) than a step in gradient descent.

[] **\nabla _{\theta } R_ n(\theta )**  is a sum of functions, one per data point.

**Ans:** 

