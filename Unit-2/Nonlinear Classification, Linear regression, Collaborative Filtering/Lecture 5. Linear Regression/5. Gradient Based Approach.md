# **Learning Algorithm: Gradient Based Approach**

**True or False**

Let **R_n(/theta)** be the least squares criterion defined by

**R_ n(\theta )=1/n \sum _{t=1}^{n} \text {Loss}\left(y^{(t)} - \theta \cdot x^{(t)}\right).**

Which of the following is true? Choose all those apply.

[*] The least squares criterion **R_n(/theta)** is a sum of functions, one per data point.

[ ] Each step in stochastic gradient descent requires more computational resources (say, as measured by counting floating point operations) than a step in gradient descent.

[*] **\nabla _{\theta } R_ n(\theta )**  is a sum of functions, one per data point.

**Ans:** 

Option 1 & 3

**Solution:**

For every point, the loss is a function of , so the least squares criterion **R_n(/theta)** is a sum of functions, one per data point, and this is what makes stochastic gradient descent possible. We want to do stochastic gradient descent because each step in it is faster than a step in gradient descent. Finally, because **R_n(/theta)** is sum of functions, one per data point, **\nabla _{\theta } R_ n(\theta )** is also a sum of functions one per data point.


Remark on SGD versus GD: Stochastic gradient descent (SGD) and gradient descent (GD) differ in their batch sizes. For "very" convex problems, GD can converge in fewer steps than SGD (with the same timesteps for fair comparison, and also taking moving averages of SGD's answer, for fair comparison). But for general nonlinear problems, we can't make a universal claim about which takes fewer steps to converge. For example, the randomness in SGD can help us escape critical points such as saddle points that would get GD stuck. “Topological" considerations suggest that such saddle points pepper the learning landscape of popular models such deep neural networks (sit tight for Unit 3 to learn about these!) We do know that each step of SGD is faster than a step of GD–potentially by orders of magnitude depending on the ratio of training-set-size to batch-size.
