# Regularization: extreme case 1

As in the video above, define the loss function

**J_{n, \lambda } (\theta , \theta _0) = \frac{1}{n} \sum _{t=1}^{n} \frac{(y^{(t)} - \theta \cdot x^{(t)}-\theta _0)^2}{2} + \frac{\lambda }{2} \left\|  \theta  \right\| ^2**

where **/lambda** is the regularization factor.

![image](https://github.com/iamkushvanth/MIT_6.86x/assets/160105601/9ac0b054-f22b-4603-ba89-6d529d29d3b9)

In the figure above, the blue dots are the training examples. If we increase **/lambda** to **/infty** ,so that **/theta** and **/theta_0**  (shorthand for the weights optimal with respect to **/lambda** and the depicted data) also change, then to which line does the predictor line (i.e., the graph of the function **f(x) = \theta \cdot x + \theta _0**  from Xs to Ys) converge?

[*] line 1

[ ] line 2

[ ] line 3

**Ans:** line 1

**Solution:**

minimizing **J** is equivalent to minimizing **norm(/theta)**. Thus  will have to be a zero vector

becomes ,**f(x) = \theta _0** a horizontal line. Thus **f(x)** converges to line 1.

# Regularization: Extreme case 2

As in the problem above,

**J_{n, \lambda } (\theta , \theta _0) = \frac{1}{n} \sum _{t=1}^{n} \frac{(y^{(t)} - \theta \cdot x^{(t)}-\theta _0)^2}{2} + \frac{\lambda }{2} \left\|  \theta  \right\| ^2**

where **/lambda** is the regularization factor.

![image](https://github.com/iamkushvanth/MIT_6.86x/assets/160105601/7eb19803-4cd0-47de-879a-e2f9b354d82b)

In the figure above, the blue dots are the training examples. If we Decrease **/lambda** to **/infty** ,so that **/theta** and **/theta_0**  (shorthand for the weights optimal with respect to **/lambda** and the depicted data) also change, then to which line does the predictor line (i.e., the graph of the function **f(x) = \theta \cdot x + \theta _0**  from Xs to Ys) converge?

[ ] line 1

[*] line 2

[ ] line 3

**Ans:** line 2


