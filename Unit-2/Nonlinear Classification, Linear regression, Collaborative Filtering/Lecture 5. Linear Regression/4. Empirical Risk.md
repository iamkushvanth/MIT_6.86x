# **Compute Hinge Loss**

The empirical risk **\( R_n(\theta) \)** is defined as

**\[ R_n(\theta) = \frac{1}{n} \sum_{t=1}^n \text{Loss}(y^t, \theta \cdot x^t) \]**

where **\( (x^t, y^t) \) is the \( t \)-th** training example (and there are **\( n \)** in total), and Loss is some loss function, such as hinge loss.

Recall from a previous lecture that the definition of hinge loss:

**\[ \text{Loss}_\text{hinge}(z) = \begin{cases} 0 & \text{if } z \geq 1 \\ 1 - z & \text{otherwise} \end{cases} \]**

In this problem, we calculate the empirical risk with hinge loss when given specific **\( \theta \) and \( \{ (x^t, y^t) \}_{t=1}^n \)**.

Assume we have 4 training examples (i.e. **\( n = 4 \)), where \( x^t \in \mathbb{R}^2 \) and \( y^t \)** is a scalar. The training examples **\( \{ (x^t, y^t) \}_{t=1,2,3,4} \)** are given as follows:


**
(x^1, y^1) & :  [1, 0, 1]^T  |  2  
(x^2, y^2) & :  [1, 1, 1]^T  |  2.7  
(x^3, y^3) & :  [1, 1, -1]^T |  -0.7  
(x^4, y^4) & :  [-1, 1, 1]^T |  2 
**


Also, we have **\( \theta = [0, 1, 2]^T \)**.

Compute the value of

**\[ R_n(\theta) = \frac{1}{4} \sum_{t=1}^4 \text{Loss}(y^t - \theta \cdot x^t) \]**

**Ans:**

5/4 or 1.25

**Solution:**

From the given table, we can calculate that

1. y^(1) - /theta . x^(1) = 0
2. y^(2) - /theta . x^(2) = -0.3
3. y^(3) - /theta . x^(3) = 0.3
4. y^(4) - /theta . x^(4) = -1

Thus we have

 Where Hinge Loss = Loss_ h(z) = 
 
                                 { 0 if Z>= 1
                                 { 1-z otherwise

  Considering that : 

  **(1+0) + (1+0.3) + (1-0.3) + (1+1) = 5**

Therefore 

  R_ n(\theta ) = 1/4 \sum _{t=1}^{4} \text {Loss}_ h(y^{(t)} - \theta \cdot x^{(t)}) = 1.25

# **Compute Squared Error Loss**

Now, we will calculate the empirical risk with the squared error loss. Remember that the squared error loss is given by

**\[ \text{Loss}(z) = \frac{z^2}{2} \]**

The 4 training examples are as in the previous problem:

**
(x^1, y^1) & :  [1, 0, 1]^T  |  2  
(x^2, y^2) & :  [1, 1, 1]^T  |  2.7  
(x^3, y^3) & :  [1, 1, -1]^T |  -0.7  
(x^4, y^4) & :  [-1, 1, 1]^T |  2 
**

As in the problem above, we have **\( \theta = [0, 1, 2]^T \)**.

Compute the value of

**\[ R_n(\theta) = 1/4 \sum_{t=1}^4 \text{Loss}(y^t - \theta \cdot x^t) \]**

**Ans:**

0.590/4

**Solution:**

**\[ R_n(\theta) = 1/4 \sum_{t=1}^4 \text{Loss}(y^t - \theta \cdot x^t) \]** = 0.1475

# **Geometrically Identifying Error**

What type of error does the figure below depict? The blue dots are the training examples, and the red line is the predictor **f(x)**

![image](https://github.com/iamkushvanth/MIT_6.86x/assets/160105601/c8a46877-5077-41b0-b91c-2e3e959edaa8)


[*] Structural error
[ ] Estimation error

**Ans:** Structural error

**Solution:**

Here, structural error occurs because the true underlying relationship is non-linear but the regression function is linear.

# **Increasing the Number of Training Examples**

If we increase **n**, the number of training examples, which of the following types of errors decreases?

[ ] Structural error
[*] Estimation error

**Ans:** Estimation error

**Solution:**

 The larger the training set is, the smaller the estimation error will be. Structural error occurs when the true underlying relationship is highly non-linear, so it is not relevant to increasing **n** .


# **Empirical Risk and Model Performance**

If we are given a large amount of training data and successfully obtain 0 empirical risk, is the model we learned guaranteed to perform well on the test data? (Assume training and test data are drawn from the same distribution.)

[ ] Yes.
[*] No.

**Ans:** NO

**Solution:**

No. Obtaining 0 empirical risk for a large amount of data still allows the possibility that the model is overfitted. For example, think of situations where we have even more parameters than training examples.






