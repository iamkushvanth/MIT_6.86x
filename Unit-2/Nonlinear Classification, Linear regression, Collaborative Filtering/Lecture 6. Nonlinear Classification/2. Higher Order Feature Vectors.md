We can use linear classifiers to make non-linear predictions. The easiest way to do this is to first map all the examples x\in **\mathbb {R}^ d** to different feature vectors **\phi (x)\in \mathbb {R}^ p** where typically **p** is much larger than **d**. We would then simply use a linear classifier on the new (higher dimensional) feature vectors, pretending that they were the original input vectors. As a result, all the linear classifiers we have learned remain applicable, yet produce non-linear classifiers in the original coordinates.

There are many ways to create such feature vectors. One common way is to use polynomial terms of the original coordinates as the components of the feature vectors. We have seen two examples in the video above. We will recall the 1-dimensional example here and see another 2-dimensional example in the problem below.

Example: Given 3 training examples with **x^{(t)}\in \mathbb {R}\, (t=1,2,3)\**, that are not linearly separable in 1-dimensional space as shown below,

![image](https://github.com/iamkushvanth/MIT_6.86x/assets/160105601/fc514427-e0d6-46e4-9b38-b5a6dd422419)

define the feature map **\, \phi (x)=\left[\phi _1(x),\phi _2(x)\right]^ T=\left[x,x^2\right]^ T,\,** which maps each  example x^(t) in 1-dimensional space to a corresponding feature vector  **\phi \left(x^{(t)}\right)=\left[x^{(t)},\left(x^{(t)}\right)^2\right]^ T**,  in 2-dimensional space.

Then, instead of using **(x^{(t)},y^{(t)})** we use **(\phi (x^{(t)}),y^{(t)})** as the training examples (where  are the labels):

![image](https://github.com/iamkushvanth/MIT_6.86x/assets/160105601/b4f923d8-c59c-4b5e-a11c-95a2bc4b14e6)

The new training set is linearly separable in the 2-dimensional **(\phi _1, \phi _2)** -space, and we can train a â€œlinear classifier" that is linear in the **\phi** -coordinates.

# Another 2-Dimensional Example

![image](https://github.com/iamkushvanth/MIT_6.86x/assets/160105601/e4726c90-2661-4724-9bee-a81bf36fc345)

Given the training examples with **\displaystyle x=\left[x_1^{(t)},x_2^{(t)}\right]\in \mathbb {R}^2** above, where a boundary between the positively-labeled examples and the negatively-labeled examples is an ellipse, which of the following feature vector(s) **\phi (x)** will guarantee that the training set **{ (\phi (x^{(t)}),y^{(t)}), t=1,.... ,n}**, (where y^(t) are the labels) are linearly separable?

**Hint:** You'll likely find it helpful to review equations for ellipses. We implicitly include bias terms, so that our decision boundaries do not have to meet the origin in feature space.

**Ans:**
